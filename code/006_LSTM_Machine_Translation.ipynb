{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "006_LSTM_Machine_Translation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0P1H3TpaGfM"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NFHnmN0_aOTh",
        "outputId": "df87676d-9bb8-4b7c-cf9f-ceecdcc172b8"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon Mar 29 12:46:30 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.56       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IEtYC2cnabGF"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import urllib3\n",
        "import zipfile\n",
        "import shutil\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "import unicodedata"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQQ3eWPVlmDh"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Embedding, Dense, Masking"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QipMdTzFyNNq"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0CBagdgmFG6"
      },
      "source": [
        "from google.colab import drive"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VcSeIe4xmLRY",
        "outputId": "bd52a6b1-86cd-4741-9690-d87f33ec1adc"
      },
      "source": [
        "drive.mount('/content/drive')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBcXS1QPrbTx"
      },
      "source": [
        "## 1. Data Load"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_f0HH8_mmOj4",
        "outputId": "c4d9d263-cb2a-48e5-caa0-91f7946c4b0c"
      },
      "source": [
        "!unzip /content/drive/My\\ Drive/Colab\\ Notebooks/Natural_Language_Processing/data/kor-eng.zip"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/drive/My Drive/Colab Notebooks/Natural_Language_Processing/data/kor-eng.zip\n",
            "  inflating: _about.txt              \n",
            "  inflating: kor.txt                 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6f_Gz8XmjPO"
      },
      "source": [
        "lines = pd.read_csv('kor.txt', names = ['src' , 'tar'],\n",
        "                    sep = '\\t',\n",
        "                    index_col = False)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "huV0Ywkgm40K",
        "outputId": "b3ed7cc4-07d8-425f-e264-223a2d1b4004"
      },
      "source": [
        "len(lines)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3648"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "FbECDmhvm5c1",
        "outputId": "7ba1a8e5-f889-425e-a0f3-bc277483d01f"
      },
      "source": [
        "lines.head()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>src</th>\n",
              "      <th>tar</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Go.</td>\n",
              "      <td>가.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Hi.</td>\n",
              "      <td>안녕.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Run!</td>\n",
              "      <td>뛰어!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Run.</td>\n",
              "      <td>뛰어.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Who?</td>\n",
              "      <td>누구?</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    src  tar\n",
              "0   Go.   가.\n",
              "1   Hi.  안녕.\n",
              "2  Run!  뛰어!\n",
              "3  Run.  뛰어.\n",
              "4  Who?  누구?"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8l26apKum886"
      },
      "source": [
        "lines.tar = lines.tar.apply(lambda x : '\\t' + x + '\\n')\n",
        "# 시작심볼, 종료심볼 추가"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "evGU-TAqnFAX",
        "outputId": "83bbd9b0-a03d-4d64-aa4b-d533e7926a2c"
      },
      "source": [
        "lines.sample(10)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>src</th>\n",
              "      <th>tar</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3112</th>\n",
              "      <td>I assure you Tom will be perfectly safe.</td>\n",
              "      <td>\\t톰은 완전 멀쩡할 거라니까.\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3194</th>\n",
              "      <td>I just don't want anyone to be mad at me.</td>\n",
              "      <td>\\t나는 그 누구도 나한테 화를 안냈으면 좋겠을 뿐이야.\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1393</th>\n",
              "      <td>What's your cat's name?</td>\n",
              "      <td>\\t네 고양이의 이름은 뭐야?\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>545</th>\n",
              "      <td>Tom confessed.</td>\n",
              "      <td>\\t톰이 자백했어.\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2785</th>\n",
              "      <td>Tom is going to be there, isn't he?</td>\n",
              "      <td>\\t톰은 거기에 가 있을 거지?\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2185</th>\n",
              "      <td>Why are you acting so stupid?</td>\n",
              "      <td>\\t넌 왜 그렇게 바보같이 굴어?\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2976</th>\n",
              "      <td>I was pretty young back in those days.</td>\n",
              "      <td>\\t그 때는 꽤 어렸었는데.\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1404</th>\n",
              "      <td>Can we still be friends?</td>\n",
              "      <td>\\t우린 아직도 친구일 수 있을까?\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>397</th>\n",
              "      <td>Examine this.</td>\n",
              "      <td>\\t이걸 조사해봐.\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2821</th>\n",
              "      <td>I caught a cold, and I have a fever.</td>\n",
              "      <td>\\t감기에 걸려서 열이 나.\\n</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            src                                tar\n",
              "3112   I assure you Tom will be perfectly safe.                \\t톰은 완전 멀쩡할 거라니까.\\n\n",
              "3194  I just don't want anyone to be mad at me.  \\t나는 그 누구도 나한테 화를 안냈으면 좋겠을 뿐이야.\\n\n",
              "1393                    What's your cat's name?                 \\t네 고양이의 이름은 뭐야?\\n\n",
              "545                              Tom confessed.                       \\t톰이 자백했어.\\n\n",
              "2785        Tom is going to be there, isn't he?                \\t톰은 거기에 가 있을 거지?\\n\n",
              "2185              Why are you acting so stupid?               \\t넌 왜 그렇게 바보같이 굴어?\\n\n",
              "2976     I was pretty young back in those days.                  \\t그 때는 꽤 어렸었는데.\\n\n",
              "1404                   Can we still be friends?              \\t우린 아직도 친구일 수 있을까?\\n\n",
              "397                               Examine this.                       \\t이걸 조사해봐.\\n\n",
              "2821       I caught a cold, and I have a fever.                  \\t감기에 걸려서 열이 나.\\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djvLoiJerrQM"
      },
      "source": [
        "## 1. 글자단위 번역기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toYjLAjDrx78"
      },
      "source": [
        "### 1) Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDQ7gMa_nG93"
      },
      "source": [
        "# 글자 집합 구축\n",
        "\n",
        "src_vocab = set()\n",
        "for line in lines.src :\n",
        "  for char in line :\n",
        "    src_vocab.add(char)\n",
        "\n",
        "tar_vocab = set()\n",
        "for line in lines.tar :\n",
        "  for char in line :\n",
        "    tar_vocab.add(char)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-Q0dyWosGbR",
        "outputId": "71932b78-9fee-41f7-ca7a-16d7deda5007"
      },
      "source": [
        "# 글자 집합의 크기 (영어, 한국어)\n",
        "src_vocab_size = len(src_vocab) + 1\n",
        "tar_vocab_size = len(tar_vocab) + 1\n",
        "\n",
        "src_vocab_size , tar_vocab_size"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(75, 912)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tAfr5RxrsPUH",
        "outputId": "8a6e355a-3f3b-4bb1-bd42-a0b36e522eea"
      },
      "source": [
        "# 정렬하여 순서를 정해준 뒤 인덱스로 내용 출력\n",
        "src_vocab = sorted(list(src_vocab))\n",
        "tar_vocab = sorted(list(tar_vocab))\n",
        "\n",
        "print(src_vocab[45:75] , '\\n' , tar_vocab[45:75])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Y', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '°', 'ï'] \n",
            " ['간', '갇', '갈', '감', '갑', '값', '갔', '강', '갖', '같', '개', '객', '갰', '걀', '걔', '거', '걱', '건', '걷', '걸', '검', '겁', '것', '게', '겐', '겠', '겨', '격', '겪', '견']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-k8Ky_js2_6"
      },
      "source": [
        "- 각 글자에 인덱스 부여"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-vW1RteAswn1",
        "outputId": "07d664b9-7aca-4528-ff43-a18d04d11bdc"
      },
      "source": [
        "src_to_index = dict([(word, i+1) for i, word in enumerate(src_vocab)])\n",
        "tar_to_index = dict([(word, i+1) for i, word in enumerate(tar_vocab)])\n",
        "\n",
        "print(src_to_index)\n",
        "print(tar_to_index)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{' ': 1, '!': 2, '\"': 3, '$': 4, '%': 5, \"'\": 6, ',': 7, '-': 8, '.': 9, '0': 10, '1': 11, '2': 12, '3': 13, '4': 14, '5': 15, '6': 16, '7': 17, '8': 18, '9': 19, ':': 20, ';': 21, '?': 22, 'A': 23, 'B': 24, 'C': 25, 'D': 26, 'E': 27, 'F': 28, 'G': 29, 'H': 30, 'I': 31, 'J': 32, 'K': 33, 'L': 34, 'M': 35, 'N': 36, 'O': 37, 'P': 38, 'Q': 39, 'R': 40, 'S': 41, 'T': 42, 'U': 43, 'V': 44, 'W': 45, 'Y': 46, 'a': 47, 'b': 48, 'c': 49, 'd': 50, 'e': 51, 'f': 52, 'g': 53, 'h': 54, 'i': 55, 'j': 56, 'k': 57, 'l': 58, 'm': 59, 'n': 60, 'o': 61, 'p': 62, 'q': 63, 'r': 64, 's': 65, 't': 66, 'u': 67, 'v': 68, 'w': 69, 'x': 70, 'y': 71, 'z': 72, '°': 73, 'ï': 74}\n",
            "{'\\t': 1, '\\n': 2, ' ': 3, '!': 4, '\"': 5, '%': 6, '(': 7, ')': 8, ',': 9, '-': 10, '.': 11, '/': 12, '0': 13, '1': 14, '2': 15, '3': 16, '4': 17, '5': 18, '6': 19, '7': 20, '8': 21, '9': 22, ':': 23, '?': 24, 'A': 25, 'B': 26, 'C': 27, 'D': 28, 'H': 29, 'M': 30, 'N': 31, 'T': 32, 'a': 33, 'd': 34, 'h': 35, 'i': 36, 'm': 37, 'o': 38, 'p': 39, 'r': 40, 't': 41, 'y': 42, '°': 43, '가': 44, '각': 45, '간': 46, '갇': 47, '갈': 48, '감': 49, '갑': 50, '값': 51, '갔': 52, '강': 53, '갖': 54, '같': 55, '개': 56, '객': 57, '갰': 58, '걀': 59, '걔': 60, '거': 61, '걱': 62, '건': 63, '걷': 64, '걸': 65, '검': 66, '겁': 67, '것': 68, '게': 69, '겐': 70, '겠': 71, '겨': 72, '격': 73, '겪': 74, '견': 75, '결': 76, '겼': 77, '경': 78, '계': 79, '고': 80, '곡': 81, '곤': 82, '곧': 83, '골': 84, '곰': 85, '곱': 86, '곳': 87, '공': 88, '과': 89, '관': 90, '광': 91, '괜': 92, '괴': 93, '굉': 94, '교': 95, '구': 96, '국': 97, '군': 98, '굳': 99, '굴': 100, '굶': 101, '굼': 102, '굽': 103, '궁': 104, '권': 105, '귀': 106, '귄': 107, '규': 108, '그': 109, '극': 110, '근': 111, '글': 112, '금': 113, '급': 114, '긋': 115, '긍': 116, '기': 117, '긴': 118, '길': 119, '깊': 120, '까': 121, '깎': 122, '깐': 123, '깔': 124, '깜': 125, '깡': 126, '깨': 127, '꺼': 128, '꺾': 129, '껍': 130, '껏': 131, '껐': 132, '께': 133, '껴': 134, '꼈': 135, '꼬': 136, '꼴': 137, '꼼': 138, '꽃': 139, '꽉': 140, '꽤': 141, '꾸': 142, '꾼': 143, '꿇': 144, '꿈': 145, '꿔': 146, '꿨': 147, '뀌': 148, '끄': 149, '끈': 150, '끊': 151, '끌': 152, '끓': 153, '끔': 154, '끗': 155, '끙': 156, '끝': 157, '끼': 158, '낀': 159, '낄': 160, '낌': 161, '나': 162, '낙': 163, '낚': 164, '난': 165, '날': 166, '낡': 167, '남': 168, '납': 169, '났': 170, '낭': 171, '낮': 172, '낯': 173, '내': 174, '낸': 175, '낼': 176, '냄': 177, '냈': 178, '냉': 179, '냐': 180, '냥': 181, '너': 182, '넌': 183, '널': 184, '넓': 185, '넘': 186, '넛': 187, '넣': 188, '네': 189, '넷': 190, '녀': 191, '녁': 192, '년': 193, '념': 194, '녕': 195, '노': 196, '녹': 197, '논': 198, '놀': 199, '농': 200, '높': 201, '놓': 202, '놔': 203, '놨': 204, '뇌': 205, '누': 206, '눅': 207, '눈': 208, '눠': 209, '눴': 210, '뉴': 211, '늄': 212, '느': 213, '늑': 214, '는': 215, '늘': 216, '늙': 217, '능': 218, '늦': 219, '니': 220, '닌': 221, '님': 222, '다': 223, '닥': 224, '닦': 225, '단': 226, '닫': 227, '달': 228, '닮': 229, '담': 230, '답': 231, '당': 232, '대': 233, '댔': 234, '더': 235, '덕': 236, '던': 237, '덜': 238, '덤': 239, '덥': 240, '데': 241, '덴': 242, '도': 243, '독': 244, '돈': 245, '돌': 246, '돕': 247, '동': 248, '돼': 249, '됐': 250, '되': 251, '된': 252, '될': 253, '됩': 254, '두': 255, '둑': 256, '둔': 257, '둘': 258, '둠': 259, '둬': 260, '뒀': 261, '뒤': 262, '드': 263, '득': 264, '든': 265, '듣': 266, '들': 267, '듯': 268, '등': 269, '디': 270, '딘': 271, '딨': 272, '따': 273, '딱': 274, '딸': 275, '땅': 276, '땋': 277, '때': 278, '떠': 279, '떡': 280, '떤': 281, '떨': 282, '떴': 283, '떻': 284, '또': 285, '똑': 286, '뚱': 287, '뛰': 288, '뜨': 289, '뜰': 290, '뜻': 291, '라': 292, '락': 293, '란': 294, '랄': 295, '람': 296, '랍': 297, '랐': 298, '랑': 299, '래': 300, '랜': 301, '램': 302, '랩': 303, '랬': 304, '략': 305, '량': 306, '러': 307, '럭': 308, '런': 309, '럴': 310, '럼': 311, '럽': 312, '렀': 313, '렁': 314, '렇': 315, '레': 316, '렌': 317, '렛': 318, '려': 319, '력': 320, '련': 321, '렵': 322, '렸': 323, '령': 324, '례': 325, '로': 326, '록': 327, '론': 328, '롭': 329, '뢰': 330, '료': 331, '루': 332, '룹': 333, '류': 334, '륙': 335, '륜': 336, '륭': 337, '르': 338, '른': 339, '를': 340, '름': 341, '릅': 342, '릎': 343, '리': 344, '린': 345, '릴': 346, '림': 347, '립': 348, '마': 349, '막': 350, '만': 351, '많': 352, '말': 353, '맙': 354, '맛': 355, '망': 356, '맞': 357, '맡': 358, '매': 359, '맥': 360, '맨': 361, '맷': 362, '머': 363, '먹': 364, '먼': 365, '멀': 366, '멈': 367, '멋': 368, '멍': 369, '메': 370, '멕': 371, '멜': 372, '며': 373, '면': 374, '명': 375, '몇': 376, '모': 377, '목': 378, '몰': 379, '몸': 380, '못': 381, '묘': 382, '무': 383, '묵': 384, '묶': 385, '문': 386, '묻': 387, '물': 388, '뭇': 389, '뭐': 390, '뭔': 391, '뭘': 392, '므': 393, '미': 394, '민': 395, '믿': 396, '밀': 397, '밌': 398, '밍': 399, '밑': 400, '바': 401, '박': 402, '밖': 403, '반': 404, '받': 405, '발': 406, '밝': 407, '밟': 408, '밤': 409, '밥': 410, '방': 411, '배': 412, '백': 413, '뱀': 414, '버': 415, '벅': 416, '번': 417, '벌': 418, '범': 419, '법': 420, '벗': 421, '벙': 422, '베': 423, '벼': 424, '벽': 425, '변': 426, '별': 427, '병': 428, '보': 429, '복': 430, '본': 431, '볼': 432, '봄': 433, '봅': 434, '봇': 435, '봉': 436, '봐': 437, '봤': 438, '부': 439, '북': 440, '분': 441, '불': 442, '붉': 443, '붐': 444, '붕': 445, '붙': 446, '브': 447, '블': 448, '비': 449, '빈': 450, '빌': 451, '빙': 452, '빛': 453, '빠': 454, '빨': 455, '빴': 456, '빵': 457, '빼': 458, '뺄': 459, '뻔': 460, '뻤': 461, '뽀': 462, '뽑': 463, '뿌': 464, '뿐': 465, '쁘': 466, '쁜': 467, '쁠': 468, '삐': 469, '사': 470, '삭': 471, '산': 472, '살': 473, '삶': 474, '삼': 475, '샀': 476, '상': 477, '새': 478, '색': 479, '샌': 480, '생': 481, '샤': 482, '샴': 483, '서': 484, '석': 485, '선': 486, '설': 487, '섬': 488, '섭': 489, '섯': 490, '섰': 491, '성': 492, '세': 493, '센': 494, '셈': 495, '셔': 496, '셜': 497, '셨': 498, '소': 499, '속': 500, '손': 501, '솔': 502, '송': 503, '쇠': 504, '수': 505, '숙': 506, '순': 507, '숟': 508, '술': 509, '숨': 510, '쉬': 511, '쉽': 512, '슈': 513, '스': 514, '슨': 515, '슬': 516, '습': 517, '승': 518, '시': 519, '식': 520, '신': 521, '실': 522, '싫': 523, '심': 524, '십': 525, '싱': 526, '싶': 527, '싸': 528, '쌉': 529, '써': 530, '썩': 531, '썼': 532, '썽': 533, '쏘': 534, '쏠': 535, '쏴': 536, '쓰': 537, '쓱': 538, '쓴': 539, '쓸': 540, '씀': 541, '씨': 542, '씩': 543, '씬': 544, '씻': 545, '아': 546, '악': 547, '안': 548, '앉': 549, '않': 550, '알': 551, '앓': 552, '암': 553, '압': 554, '앗': 555, '았': 556, '앙': 557, '앞': 558, '애': 559, '액': 560, '앨': 561, '앵': 562, '야': 563, '약': 564, '얀': 565, '얇': 566, '양': 567, '얗': 568, '얘': 569, '어': 570, '억': 571, '언': 572, '얻': 573, '얼': 574, '엄': 575, '업': 576, '없': 577, '엇': 578, '었': 579, '엌': 580, '에': 581, '엔': 582, '엘': 583, '여': 584, '역': 585, '연': 586, '열': 587, '염': 588, '엽': 589, '였': 590, '영': 591, '옆': 592, '예': 593, '옛': 594, '오': 595, '옥': 596, '온': 597, '올': 598, '옮': 599, '옳': 600, '옷': 601, '옹': 602, '와': 603, '완': 604, '왔': 605, '왜': 606, '외': 607, '왼': 608, '요': 609, '욕': 610, '용': 611, '우': 612, '운': 613, '울': 614, '움': 615, '웃': 616, '워': 617, '원': 618, '월': 619, '웠': 620, '웨': 621, '위': 622, '윈': 623, '윗': 624, '윙': 625, '유': 626, '육': 627, '윤': 628, '으': 629, '은': 630, '을': 631, '음': 632, '읍': 633, '응': 634, '의': 635, '이': 636, '익': 637, '인': 638, '일': 639, '읽': 640, '잃': 641, '임': 642, '입': 643, '있': 644, '잊': 645, '자': 646, '작': 647, '잔': 648, '잖': 649, '잘': 650, '잠': 651, '잡': 652, '잤': 653, '장': 654, '재': 655, '잭': 656, '쟁': 657, '저': 658, '적': 659, '전': 660, '절': 661, '젊': 662, '점': 663, '접': 664, '정': 665, '제': 666, '젝': 667, '젠': 668, '젯': 669, '져': 670, '졌': 671, '조': 672, '족': 673, '존': 674, '졸': 675, '좀': 676, '종': 677, '좋': 678, '좌': 679, '죄': 680, '죠': 681, '주': 682, '죽': 683, '준': 684, '줄': 685, '중': 686, '줘': 687, '줬': 688, '쥐': 689, '즈': 690, '즉': 691, '즐': 692, '즘': 693, '증': 694, '지': 695, '직': 696, '진': 697, '질': 698, '집': 699, '짓': 700, '짖': 701, '짜': 702, '짝': 703, '짧': 704, '째': 705, '쨌': 706, '쩔': 707, '쩡': 708, '쪄': 709, '쪘': 710, '쪼': 711, '쪽': 712, '쫓': 713, '쯤': 714, '찌': 715, '찍': 716, '찔': 717, '찡': 718, '찢': 719, '차': 720, '착': 721, '찬': 722, '찮': 723, '찰': 724, '참': 725, '찼': 726, '창': 727, '찾': 728, '채': 729, '책': 730, '챌': 731, '챘': 732, '처': 733, '척': 734, '천': 735, '철': 736, '첫': 737, '청': 738, '체': 739, '쳐': 740, '쳤': 741, '초': 742, '촌': 743, '총': 744, '최': 745, '추': 746, '축': 747, '출': 748, '춤': 749, '충': 750, '춰': 751, '췄': 752, '취': 753, '츠': 754, '측': 755, '치': 756, '칙': 757, '친': 758, '칠': 759, '침': 760, '칩': 761, '카': 762, '캐': 763, '커': 764, '컨': 765, '컴': 766, '컵': 767, '케': 768, '켓': 769, '켜': 770, '켤': 771, '켰': 772, '코': 773, '콜': 774, '콥': 775, '콩': 776, '쾅': 777, '쿠': 778, '쿨': 779, '퀴': 780, '큐': 781, '크': 782, '큰': 783, '큼': 784, '키': 785, '킬': 786, '킹': 787, '타': 788, '탁': 789, '탄': 790, '탈': 791, '탐': 792, '탑': 793, '탓': 794, '탔': 795, '탕': 796, '태': 797, '택': 798, '터': 799, '턱': 800, '턴': 801, '테': 802, '텐': 803, '토': 804, '톰': 805, '톱': 806, '통': 807, '퇴': 808, '투': 809, '트': 810, '특': 811, '튼': 812, '틀': 813, '티': 814, '틱': 815, '틴': 816, '팀': 817, '팅': 818, '파': 819, '판': 820, '팔': 821, '팠': 822, '패': 823, '퍼': 824, '펐': 825, '페': 826, '펙': 827, '펜': 828, '펭': 829, '펴': 830, '편': 831, '평': 832, '폐': 833, '포': 834, '폭': 835, '폰': 836, '표': 837, '푸': 838, '푹': 839, '풀': 840, '품': 841, '풍': 842, '퓨': 843, '프': 844, '픈': 845, '플': 846, '픔': 847, '피': 848, '필': 849, '핑': 850, '하': 851, '학': 852, '한': 853, '할': 854, '함': 855, '합': 856, '항': 857, '해': 858, '핸': 859, '햄': 860, '했': 861, '행': 862, '향': 863, '햿': 864, '허': 865, '헉': 866, '헌': 867, '험': 868, '헤': 869, '헬': 870, '헷': 871, '혀': 872, '현': 873, '혈': 874, '혐': 875, '협': 876, '혔': 877, '형': 878, '혜': 879, '호': 880, '혹': 881, '혼': 882, '홉': 883, '홋': 884, '화': 885, '확': 886, '환': 887, '활': 888, '황': 889, '회': 890, '획': 891, '효': 892, '후': 893, '훈': 894, '훌': 895, '훔': 896, '훨': 897, '휘': 898, '휴': 899, '흐': 900, '흔': 901, '흘': 902, '흙': 903, '흠': 904, '흡': 905, '흥': 906, '희': 907, '흰': 908, '히': 909, '힌': 910, '힘': 911}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7c2qnAGazdGM"
      },
      "source": [
        "- 정수 인코딩"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4M3ptFWmzD6Z"
      },
      "source": [
        "# 영어문장에 대한 정수인코딩\n",
        "encoder_input = []\n",
        "\n",
        "for line in lines.src:\n",
        "  temp_X = []\n",
        "  for w in line :\n",
        "    temp_X.append(src_to_index[w])\n",
        "  encoder_input.append(temp_X)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zotfYzR_zs0z",
        "outputId": "fdbed861-511a-4750-d873-c2c23bbb90c8"
      },
      "source": [
        "print(encoder_input[:5])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[29, 61, 9], [30, 55, 9], [40, 67, 60, 2], [40, 67, 60, 9], [45, 54, 61, 22]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhqgJzCBzuJr"
      },
      "source": [
        "# 한국어 데이터에 대한 정수인코딩\n",
        "decoder_input = []\n",
        "\n",
        "for line in lines.tar:\n",
        "  temp_X = []\n",
        "  for w in line:\n",
        "    temp_X.append(tar_to_index[w])\n",
        "  decoder_input.append(temp_X)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5X256VDUz-5e",
        "outputId": "e0cc0c3b-5b9c-4825-b211-7dbe15f672a6"
      },
      "source": [
        "print(decoder_input[:5])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1, 44, 11, 2], [1, 548, 195, 11, 2], [1, 288, 570, 4, 2], [1, 288, 570, 11, 2], [1, 206, 96, 24, 2]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQeQd1Dt0AbB"
      },
      "source": [
        "# 실제값 정수인코딩\n",
        "decoder_target = []\n",
        "\n",
        "for line in lines.tar:\n",
        "  t = 0\n",
        "  temp_X = []\n",
        "  for w in line:\n",
        "    if t > 0:\n",
        "      temp_X.append(tar_to_index[w])\n",
        "    t += 1\n",
        "  decoder_target.append(temp_X)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OeIrVaEB0bdd",
        "outputId": "55b0e4a0-2d61-43e4-fa69-74ee443a6d0d"
      },
      "source": [
        "print(decoder_target[:5])"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[44, 11, 2], [548, 195, 11, 2], [288, 570, 4, 2], [288, 570, 11, 2], [206, 96, 24, 2]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "teIlivgz0zLE"
      },
      "source": [
        "- 패딩작업"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6fe0cNA0c5F"
      },
      "source": [
        "max_src_len = max([len(line) for line in lines.src])\n",
        "max_tar_len = max([len(line) for line in lines.tar])"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iy3C99Aq1IgW",
        "outputId": "1b39fbbe-aa73-41d0-d229-654de05f9a63"
      },
      "source": [
        "max_src_len, max_tar_len"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(537, 298)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nW-MjoG91KnG"
      },
      "source": [
        "# 영어는 영어 길이, 한국어는 한국어 길이에 맞춰 패딩\n",
        "encoder_input = pad_sequences(encoder_input,\n",
        "                              maxlen = max_src_len,\n",
        "                              padding = 'post')\n",
        "decoder_input = pad_sequences(decoder_input,\n",
        "                              maxlen = max_tar_len,\n",
        "                              padding = 'post')\n",
        "decoder_target = pad_sequences(decoder_target,\n",
        "                               maxlen = max_tar_len,\n",
        "                               padding = 'post')"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UAa5MiU1pOS"
      },
      "source": [
        "- 원핫인코딩\n",
        "  - 글자단위 번역기므로 워드임베딩 사용하지 않음\n",
        "  - 입력값도 원핫벡터 사용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63LpEJvS1oum"
      },
      "source": [
        "encoder_input = to_categorical(encoder_input)\n",
        "decoder_input = to_categorical(decoder_input)\n",
        "decoder_target = to_categorical(decoder_target)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUrqrNzk2ASX"
      },
      "source": [
        "### 2) 모델 설계, 훈련"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qsqLKfx419vI"
      },
      "source": [
        "encoder_inputs = Input(shape = (None, src_vocab_size))\n",
        "encoder_lstm = LSTM(units = 256, return_state = True) # 인코더의 내부 상태를 디코더로 넘겨줌\n",
        "encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs)\n",
        "\n",
        "# encoder_outputs는 리턴받았지만 필요없으므로 사용하지 않음\n",
        "\n",
        "encoder_states = [state_h, state_c]\n",
        "# LSTM은 RNN과 달리 상태가 두개(은닉 상태, 셀 상태)\n",
        "# 두가지 상태를 모두 디코더로 전달(컨텍스트 벡터)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BiolddZ32zuH"
      },
      "source": [
        "decoder_inputs = Input(shape = (None, tar_vocab_size))\n",
        "decoder_lstm = LSTM(units = 256,\n",
        "                    return_sequences = True,\n",
        "                    return_state = True)\n",
        "\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
        "                                     initial_state = encoder_states)\n",
        "# 디코더의 첫 상태를 인코더의 은닉상태, 셀 상태로 지정\n",
        "\n",
        "decoder_softmax_layer = Dense(tar_vocab_size,\n",
        "                              activation = 'softmax')\n",
        "decoder_outputs = decoder_softmax_layer(decoder_outputs)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYyTTcWx4T5D"
      },
      "source": [
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4pkJOjp4Yi9"
      },
      "source": [
        "model.compile(optimizer = 'adam',\n",
        "              loss = 'categorical_crossentropy')"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ngeoKfFa4dN9",
        "outputId": "ae12157d-233e-4425-ffa0-d08e59482618"
      },
      "source": [
        "model.fit(x = [encoder_input, decoder_input],\n",
        "          y = decoder_target,\n",
        "          batch_size = 64,\n",
        "          epochs = 200,\n",
        "          validation_split = .2)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "46/46 [==============================] - 41s 205ms/step - loss: 3.0841 - val_loss: 0.4909\n",
            "Epoch 2/200\n",
            "46/46 [==============================] - 8s 180ms/step - loss: 0.2392 - val_loss: 0.4833\n",
            "Epoch 3/200\n",
            "46/46 [==============================] - 8s 178ms/step - loss: 0.2324 - val_loss: 0.4665\n",
            "Epoch 4/200\n",
            "46/46 [==============================] - 8s 180ms/step - loss: 0.2271 - val_loss: 0.4587\n",
            "Epoch 5/200\n",
            "46/46 [==============================] - 8s 179ms/step - loss: 0.2147 - val_loss: 0.4622\n",
            "Epoch 6/200\n",
            "46/46 [==============================] - 8s 178ms/step - loss: 0.2046 - val_loss: 0.4091\n",
            "Epoch 7/200\n",
            "46/46 [==============================] - 8s 181ms/step - loss: 0.1960 - val_loss: 0.3915\n",
            "Epoch 8/200\n",
            "46/46 [==============================] - 8s 176ms/step - loss: 0.1881 - val_loss: 0.3840\n",
            "Epoch 9/200\n",
            "46/46 [==============================] - 8s 178ms/step - loss: 0.1797 - val_loss: 0.3755\n",
            "Epoch 10/200\n",
            "46/46 [==============================] - 8s 177ms/step - loss: 0.1790 - val_loss: 0.3680\n",
            "Epoch 11/200\n",
            "46/46 [==============================] - 8s 182ms/step - loss: 0.1695 - val_loss: 0.3599\n",
            "Epoch 12/200\n",
            "46/46 [==============================] - 8s 180ms/step - loss: 0.1688 - val_loss: 0.3611\n",
            "Epoch 13/200\n",
            "46/46 [==============================] - 8s 180ms/step - loss: 0.1638 - val_loss: 0.3541\n",
            "Epoch 14/200\n",
            "46/46 [==============================] - 8s 177ms/step - loss: 0.1601 - val_loss: 0.3495\n",
            "Epoch 15/200\n",
            "46/46 [==============================] - 8s 179ms/step - loss: 0.1576 - val_loss: 0.3458\n",
            "Epoch 16/200\n",
            "46/46 [==============================] - 8s 178ms/step - loss: 0.1558 - val_loss: 0.3306\n",
            "Epoch 17/200\n",
            "46/46 [==============================] - 8s 177ms/step - loss: 0.1543 - val_loss: 0.3406\n",
            "Epoch 18/200\n",
            "46/46 [==============================] - 8s 181ms/step - loss: 0.1486 - val_loss: 0.3359\n",
            "Epoch 19/200\n",
            "46/46 [==============================] - 8s 180ms/step - loss: 0.1492 - val_loss: 0.3435\n",
            "Epoch 20/200\n",
            "46/46 [==============================] - 8s 179ms/step - loss: 0.1460 - val_loss: 0.3362\n",
            "Epoch 21/200\n",
            "46/46 [==============================] - 8s 181ms/step - loss: 0.1426 - val_loss: 0.3240\n",
            "Epoch 22/200\n",
            "46/46 [==============================] - 8s 180ms/step - loss: 0.1403 - val_loss: 0.3261\n",
            "Epoch 23/200\n",
            "46/46 [==============================] - 8s 180ms/step - loss: 0.1362 - val_loss: 0.3087\n",
            "Epoch 24/200\n",
            "46/46 [==============================] - 8s 180ms/step - loss: 0.1388 - val_loss: 0.3162\n",
            "Epoch 25/200\n",
            "46/46 [==============================] - 8s 178ms/step - loss: 0.1363 - val_loss: 0.3509\n",
            "Epoch 26/200\n",
            "46/46 [==============================] - 8s 179ms/step - loss: 0.2095 - val_loss: 0.3283\n",
            "Epoch 27/200\n",
            "46/46 [==============================] - 8s 181ms/step - loss: 0.1408 - val_loss: 0.3253\n",
            "Epoch 28/200\n",
            "46/46 [==============================] - 8s 183ms/step - loss: 0.1291 - val_loss: 0.3097\n",
            "Epoch 29/200\n",
            "46/46 [==============================] - 8s 179ms/step - loss: 0.1269 - val_loss: 0.3154\n",
            "Epoch 30/200\n",
            "46/46 [==============================] - 8s 181ms/step - loss: 0.1265 - val_loss: 0.3256\n",
            "Epoch 31/200\n",
            "46/46 [==============================] - 8s 181ms/step - loss: 0.1249 - val_loss: 0.3166\n",
            "Epoch 32/200\n",
            "46/46 [==============================] - 8s 179ms/step - loss: 0.1227 - val_loss: 0.3169\n",
            "Epoch 33/200\n",
            "46/46 [==============================] - 8s 178ms/step - loss: 0.1233 - val_loss: 0.3188\n",
            "Epoch 34/200\n",
            "46/46 [==============================] - 8s 179ms/step - loss: 0.1224 - val_loss: 0.3096\n",
            "Epoch 35/200\n",
            "46/46 [==============================] - 8s 177ms/step - loss: 0.1196 - val_loss: 0.3045\n",
            "Epoch 36/200\n",
            "46/46 [==============================] - 8s 180ms/step - loss: 0.1184 - val_loss: 0.3064\n",
            "Epoch 37/200\n",
            "46/46 [==============================] - 8s 178ms/step - loss: 0.1156 - val_loss: 0.3205\n",
            "Epoch 38/200\n",
            "46/46 [==============================] - 8s 180ms/step - loss: 0.1160 - val_loss: 0.3119\n",
            "Epoch 39/200\n",
            "46/46 [==============================] - 8s 178ms/step - loss: 0.1130 - val_loss: 0.3174\n",
            "Epoch 40/200\n",
            "46/46 [==============================] - 8s 179ms/step - loss: 0.1130 - val_loss: 0.3268\n",
            "Epoch 41/200\n",
            "46/46 [==============================] - 8s 181ms/step - loss: 0.1122 - val_loss: 0.3154\n",
            "Epoch 42/200\n",
            "46/46 [==============================] - 8s 180ms/step - loss: 0.1103 - val_loss: 0.3069\n",
            "Epoch 43/200\n",
            "46/46 [==============================] - 8s 180ms/step - loss: 0.1085 - val_loss: 0.3070\n",
            "Epoch 44/200\n",
            "46/46 [==============================] - 8s 178ms/step - loss: 0.1078 - val_loss: 0.2998\n",
            "Epoch 45/200\n",
            "46/46 [==============================] - 8s 178ms/step - loss: 0.1064 - val_loss: 0.2861\n",
            "Epoch 46/200\n",
            "46/46 [==============================] - 8s 180ms/step - loss: 0.1045 - val_loss: 0.3081\n",
            "Epoch 47/200\n",
            "46/46 [==============================] - 8s 181ms/step - loss: 0.1036 - val_loss: 0.2955\n",
            "Epoch 48/200\n",
            "46/46 [==============================] - 8s 180ms/step - loss: 0.1035 - val_loss: 0.3110\n",
            "Epoch 49/200\n",
            "46/46 [==============================] - 8s 176ms/step - loss: 0.1020 - val_loss: 0.3176\n",
            "Epoch 50/200\n",
            "46/46 [==============================] - 8s 179ms/step - loss: 0.1017 - val_loss: 0.3101\n",
            "Epoch 51/200\n",
            "46/46 [==============================] - 8s 180ms/step - loss: 0.0997 - val_loss: 0.3092\n",
            "Epoch 52/200\n",
            "46/46 [==============================] - 8s 182ms/step - loss: 0.0979 - val_loss: 0.3054\n",
            "Epoch 53/200\n",
            "46/46 [==============================] - 8s 180ms/step - loss: 0.0973 - val_loss: 0.3103\n",
            "Epoch 54/200\n",
            "46/46 [==============================] - 8s 180ms/step - loss: 0.0967 - val_loss: 0.3099\n",
            "Epoch 55/200\n",
            "46/46 [==============================] - 8s 178ms/step - loss: 0.0947 - val_loss: 0.3065\n",
            "Epoch 56/200\n",
            "46/46 [==============================] - 8s 180ms/step - loss: 0.0948 - val_loss: 0.3211\n",
            "Epoch 57/200\n",
            "46/46 [==============================] - 8s 182ms/step - loss: 0.0934 - val_loss: 0.3190\n",
            "Epoch 58/200\n",
            "46/46 [==============================] - 8s 179ms/step - loss: 0.0936 - val_loss: 0.3085\n",
            "Epoch 59/200\n",
            "46/46 [==============================] - 8s 179ms/step - loss: 0.0910 - val_loss: 0.3087\n",
            "Epoch 60/200\n",
            "46/46 [==============================] - 8s 180ms/step - loss: 0.0905 - val_loss: 0.3119\n",
            "Epoch 61/200\n",
            "46/46 [==============================] - 8s 180ms/step - loss: 0.0897 - val_loss: 0.3195\n",
            "Epoch 62/200\n",
            "46/46 [==============================] - 8s 179ms/step - loss: 0.0890 - val_loss: 0.3212\n",
            "Epoch 63/200\n",
            "46/46 [==============================] - 8s 177ms/step - loss: 0.0882 - val_loss: 0.3219\n",
            "Epoch 64/200\n",
            "46/46 [==============================] - 8s 178ms/step - loss: 0.0873 - val_loss: 0.3236\n",
            "Epoch 65/200\n",
            "46/46 [==============================] - 8s 179ms/step - loss: 0.0857 - val_loss: 0.2983\n",
            "Epoch 66/200\n",
            "46/46 [==============================] - 8s 180ms/step - loss: 0.0847 - val_loss: 0.3098\n",
            "Epoch 67/200\n",
            "46/46 [==============================] - 8s 181ms/step - loss: 0.0836 - val_loss: 0.3203\n",
            "Epoch 68/200\n",
            "46/46 [==============================] - 8s 179ms/step - loss: 0.0843 - val_loss: 0.3169\n",
            "Epoch 69/200\n",
            "46/46 [==============================] - 8s 181ms/step - loss: 0.0826 - val_loss: 0.3182\n",
            "Epoch 70/200\n",
            "46/46 [==============================] - 8s 180ms/step - loss: 0.0803 - val_loss: 0.3284\n",
            "Epoch 71/200\n",
            "46/46 [==============================] - 8s 180ms/step - loss: 0.0804 - val_loss: 0.3231\n",
            "Epoch 72/200\n",
            "46/46 [==============================] - 8s 179ms/step - loss: 0.0801 - val_loss: 0.3251\n",
            "Epoch 73/200\n",
            "46/46 [==============================] - 8s 179ms/step - loss: 0.0783 - val_loss: 0.3226\n",
            "Epoch 74/200\n",
            "46/46 [==============================] - 8s 180ms/step - loss: 0.0773 - val_loss: 0.3261\n",
            "Epoch 75/200\n",
            "46/46 [==============================] - 8s 179ms/step - loss: 0.0770 - val_loss: 0.3374\n",
            "Epoch 76/200\n",
            "46/46 [==============================] - 8s 176ms/step - loss: 0.0764 - val_loss: 0.3315\n",
            "Epoch 77/200\n",
            "46/46 [==============================] - 8s 181ms/step - loss: 0.0750 - val_loss: 0.3190\n",
            "Epoch 78/200\n",
            "46/46 [==============================] - 8s 180ms/step - loss: 0.0741 - val_loss: 0.3331\n",
            "Epoch 79/200\n",
            "46/46 [==============================] - 8s 181ms/step - loss: 0.0731 - val_loss: 0.3495\n",
            "Epoch 80/200\n",
            "46/46 [==============================] - 8s 177ms/step - loss: 0.0736 - val_loss: 0.3388\n",
            "Epoch 81/200\n",
            "46/46 [==============================] - 8s 179ms/step - loss: 0.0726 - val_loss: 0.3361\n",
            "Epoch 82/200\n",
            "46/46 [==============================] - 8s 179ms/step - loss: 0.0707 - val_loss: 0.3396\n",
            "Epoch 83/200\n",
            "46/46 [==============================] - 8s 176ms/step - loss: 0.0706 - val_loss: 0.3306\n",
            "Epoch 84/200\n",
            "46/46 [==============================] - 8s 181ms/step - loss: 0.0697 - val_loss: 0.3319\n",
            "Epoch 85/200\n",
            "46/46 [==============================] - 8s 178ms/step - loss: 0.0700 - val_loss: 0.3378\n",
            "Epoch 86/200\n",
            "46/46 [==============================] - 8s 180ms/step - loss: 0.0691 - val_loss: 0.3379\n",
            "Epoch 87/200\n",
            "46/46 [==============================] - 8s 180ms/step - loss: 0.0682 - val_loss: 0.3556\n",
            "Epoch 88/200\n",
            "46/46 [==============================] - 8s 180ms/step - loss: 0.0677 - val_loss: 0.3410\n",
            "Epoch 89/200\n",
            "46/46 [==============================] - 8s 178ms/step - loss: 0.0660 - val_loss: 0.3541\n",
            "Epoch 90/200\n",
            "46/46 [==============================] - 8s 179ms/step - loss: 0.0661 - val_loss: 0.3350\n",
            "Epoch 91/200\n",
            "46/46 [==============================] - 8s 180ms/step - loss: 0.0652 - val_loss: 0.3511\n",
            "Epoch 92/200\n",
            "46/46 [==============================] - 8s 178ms/step - loss: 0.0640 - val_loss: 0.3465\n",
            "Epoch 93/200\n",
            "46/46 [==============================] - 8s 178ms/step - loss: 0.0637 - val_loss: 0.3415\n",
            "Epoch 94/200\n",
            "46/46 [==============================] - 8s 177ms/step - loss: 0.0638 - val_loss: 0.3518\n",
            "Epoch 95/200\n",
            "46/46 [==============================] - 8s 178ms/step - loss: 0.0629 - val_loss: 0.3537\n",
            "Epoch 96/200\n",
            "46/46 [==============================] - 8s 182ms/step - loss: 0.0620 - val_loss: 0.3504\n",
            "Epoch 97/200\n",
            "46/46 [==============================] - 8s 179ms/step - loss: 0.0619 - val_loss: 0.3615\n",
            "Epoch 98/200\n",
            "46/46 [==============================] - 8s 180ms/step - loss: 0.0604 - val_loss: 0.3560\n",
            "Epoch 99/200\n",
            "46/46 [==============================] - 8s 182ms/step - loss: 0.0601 - val_loss: 0.3629\n",
            "Epoch 100/200\n",
            "46/46 [==============================] - 8s 179ms/step - loss: 0.0601 - val_loss: 0.3568\n",
            "Epoch 101/200\n",
            "46/46 [==============================] - 8s 182ms/step - loss: 0.0590 - val_loss: 0.3566\n",
            "Epoch 102/200\n",
            "46/46 [==============================] - 8s 182ms/step - loss: 0.0584 - val_loss: 0.3666\n",
            "Epoch 103/200\n",
            "46/46 [==============================] - 8s 179ms/step - loss: 0.0581 - val_loss: 0.3591\n",
            "Epoch 104/200\n",
            "46/46 [==============================] - 8s 180ms/step - loss: 0.0571 - val_loss: 0.3623\n",
            "Epoch 105/200\n",
            "46/46 [==============================] - 8s 180ms/step - loss: 0.0561 - val_loss: 0.3659\n",
            "Epoch 106/200\n",
            "46/46 [==============================] - 8s 178ms/step - loss: 0.0561 - val_loss: 0.3437\n",
            "Epoch 107/200\n",
            "46/46 [==============================] - 8s 180ms/step - loss: 0.0555 - val_loss: 0.3603\n",
            "Epoch 108/200\n",
            "46/46 [==============================] - 8s 183ms/step - loss: 0.0547 - val_loss: 0.3492\n",
            "Epoch 109/200\n",
            "46/46 [==============================] - 8s 179ms/step - loss: 0.0544 - val_loss: 0.3637\n",
            "Epoch 110/200\n",
            "46/46 [==============================] - 8s 179ms/step - loss: 0.0543 - val_loss: 0.3664\n",
            "Epoch 111/200\n",
            "46/46 [==============================] - 8s 179ms/step - loss: 0.0536 - val_loss: 0.3630\n",
            "Epoch 112/200\n",
            "46/46 [==============================] - 8s 182ms/step - loss: 0.0532 - val_loss: 0.3591\n",
            "Epoch 113/200\n",
            "46/46 [==============================] - 8s 178ms/step - loss: 0.0523 - val_loss: 0.3717\n",
            "Epoch 114/200\n",
            "46/46 [==============================] - 8s 183ms/step - loss: 0.0520 - val_loss: 0.3770\n",
            "Epoch 115/200\n",
            "46/46 [==============================] - 8s 181ms/step - loss: 0.0517 - val_loss: 0.3798\n",
            "Epoch 116/200\n",
            "46/46 [==============================] - 8s 181ms/step - loss: 0.0503 - val_loss: 0.3741\n",
            "Epoch 117/200\n",
            "46/46 [==============================] - 8s 178ms/step - loss: 0.0505 - val_loss: 0.3874\n",
            "Epoch 118/200\n",
            "46/46 [==============================] - 8s 180ms/step - loss: 0.0508 - val_loss: 0.3653\n",
            "Epoch 119/200\n",
            "46/46 [==============================] - 8s 181ms/step - loss: 0.0502 - val_loss: 0.3823\n",
            "Epoch 120/200\n",
            "46/46 [==============================] - 8s 177ms/step - loss: 0.0491 - val_loss: 0.3891\n",
            "Epoch 121/200\n",
            "46/46 [==============================] - 8s 182ms/step - loss: 0.0485 - val_loss: 0.3836\n",
            "Epoch 122/200\n",
            "46/46 [==============================] - 8s 182ms/step - loss: 0.0482 - val_loss: 0.3915\n",
            "Epoch 123/200\n",
            "46/46 [==============================] - 8s 178ms/step - loss: 0.0478 - val_loss: 0.3922\n",
            "Epoch 124/200\n",
            "46/46 [==============================] - 8s 181ms/step - loss: 0.0476 - val_loss: 0.3789\n",
            "Epoch 125/200\n",
            "46/46 [==============================] - 8s 180ms/step - loss: 0.0470 - val_loss: 0.3855\n",
            "Epoch 126/200\n",
            "46/46 [==============================] - 8s 183ms/step - loss: 0.0469 - val_loss: 0.3846\n",
            "Epoch 127/200\n",
            "46/46 [==============================] - 8s 179ms/step - loss: 0.0463 - val_loss: 0.3872\n",
            "Epoch 128/200\n",
            "46/46 [==============================] - 8s 178ms/step - loss: 0.0457 - val_loss: 0.3973\n",
            "Epoch 129/200\n",
            "46/46 [==============================] - 8s 179ms/step - loss: 0.0463 - val_loss: 0.3940\n",
            "Epoch 130/200\n",
            "46/46 [==============================] - 8s 181ms/step - loss: 0.0459 - val_loss: 0.4027\n",
            "Epoch 131/200\n",
            "46/46 [==============================] - 8s 182ms/step - loss: 0.0454 - val_loss: 0.3936\n",
            "Epoch 132/200\n",
            "46/46 [==============================] - 8s 180ms/step - loss: 0.0450 - val_loss: 0.4055\n",
            "Epoch 133/200\n",
            "46/46 [==============================] - 8s 178ms/step - loss: 0.0452 - val_loss: 0.3903\n",
            "Epoch 134/200\n",
            "46/46 [==============================] - 8s 178ms/step - loss: 0.0439 - val_loss: 0.3963\n",
            "Epoch 135/200\n",
            "46/46 [==============================] - 8s 181ms/step - loss: 0.0439 - val_loss: 0.3957\n",
            "Epoch 136/200\n",
            "46/46 [==============================] - 8s 180ms/step - loss: 0.0432 - val_loss: 0.4072\n",
            "Epoch 137/200\n",
            "46/46 [==============================] - 8s 179ms/step - loss: 0.0431 - val_loss: 0.3964\n",
            "Epoch 138/200\n",
            "46/46 [==============================] - 8s 178ms/step - loss: 0.0428 - val_loss: 0.3972\n",
            "Epoch 139/200\n",
            "46/46 [==============================] - 8s 179ms/step - loss: 0.0423 - val_loss: 0.3998\n",
            "Epoch 140/200\n",
            "46/46 [==============================] - 8s 181ms/step - loss: 0.0420 - val_loss: 0.4198\n",
            "Epoch 141/200\n",
            "46/46 [==============================] - 8s 182ms/step - loss: 0.0421 - val_loss: 0.4014\n",
            "Epoch 142/200\n",
            "46/46 [==============================] - 8s 180ms/step - loss: 0.0414 - val_loss: 0.4213\n",
            "Epoch 143/200\n",
            "46/46 [==============================] - 8s 180ms/step - loss: 0.0417 - val_loss: 0.4184\n",
            "Epoch 144/200\n",
            "46/46 [==============================] - 8s 179ms/step - loss: 0.0409 - val_loss: 0.4041\n",
            "Epoch 145/200\n",
            "46/46 [==============================] - 8s 182ms/step - loss: 0.0405 - val_loss: 0.4351\n",
            "Epoch 146/200\n",
            "46/46 [==============================] - 8s 178ms/step - loss: 0.0404 - val_loss: 0.4229\n",
            "Epoch 147/200\n",
            "46/46 [==============================] - 8s 179ms/step - loss: 0.0400 - val_loss: 0.4187\n",
            "Epoch 148/200\n",
            "46/46 [==============================] - 8s 181ms/step - loss: 0.0405 - val_loss: 0.4176\n",
            "Epoch 149/200\n",
            "46/46 [==============================] - 8s 180ms/step - loss: 0.0395 - val_loss: 0.4352\n",
            "Epoch 150/200\n",
            "46/46 [==============================] - 8s 183ms/step - loss: 0.0395 - val_loss: 0.4342\n",
            "Epoch 151/200\n",
            "46/46 [==============================] - 8s 180ms/step - loss: 0.0394 - val_loss: 0.4203\n",
            "Epoch 152/200\n",
            "46/46 [==============================] - 8s 179ms/step - loss: 0.0391 - val_loss: 0.4267\n",
            "Epoch 153/200\n",
            "46/46 [==============================] - 8s 181ms/step - loss: 0.0388 - val_loss: 0.4175\n",
            "Epoch 154/200\n",
            "46/46 [==============================] - 8s 180ms/step - loss: 0.0387 - val_loss: 0.4232\n",
            "Epoch 155/200\n",
            "46/46 [==============================] - 8s 181ms/step - loss: 0.0384 - val_loss: 0.4273\n",
            "Epoch 156/200\n",
            "46/46 [==============================] - 8s 182ms/step - loss: 0.0384 - val_loss: 0.4299\n",
            "Epoch 157/200\n",
            "46/46 [==============================] - 8s 182ms/step - loss: 0.0380 - val_loss: 0.4363\n",
            "Epoch 158/200\n",
            "46/46 [==============================] - 8s 179ms/step - loss: 0.0378 - val_loss: 0.4265\n",
            "Epoch 159/200\n",
            "46/46 [==============================] - 8s 183ms/step - loss: 0.0377 - val_loss: 0.4416\n",
            "Epoch 160/200\n",
            "46/46 [==============================] - 8s 177ms/step - loss: 0.0377 - val_loss: 0.4403\n",
            "Epoch 161/200\n",
            "46/46 [==============================] - 8s 176ms/step - loss: 0.0374 - val_loss: 0.4334\n",
            "Epoch 162/200\n",
            "46/46 [==============================] - 8s 181ms/step - loss: 0.0368 - val_loss: 0.4397\n",
            "Epoch 163/200\n",
            "46/46 [==============================] - 8s 183ms/step - loss: 0.0369 - val_loss: 0.4298\n",
            "Epoch 164/200\n",
            "46/46 [==============================] - 8s 178ms/step - loss: 0.0365 - val_loss: 0.4334\n",
            "Epoch 165/200\n",
            "46/46 [==============================] - 8s 181ms/step - loss: 0.0364 - val_loss: 0.4485\n",
            "Epoch 166/200\n",
            "46/46 [==============================] - 8s 178ms/step - loss: 0.0363 - val_loss: 0.4393\n",
            "Epoch 167/200\n",
            "46/46 [==============================] - 8s 181ms/step - loss: 0.0361 - val_loss: 0.4419\n",
            "Epoch 168/200\n",
            "46/46 [==============================] - 8s 181ms/step - loss: 0.0360 - val_loss: 0.4426\n",
            "Epoch 169/200\n",
            "46/46 [==============================] - 8s 181ms/step - loss: 0.0359 - val_loss: 0.4500\n",
            "Epoch 170/200\n",
            "46/46 [==============================] - 8s 182ms/step - loss: 0.0358 - val_loss: 0.4420\n",
            "Epoch 171/200\n",
            "46/46 [==============================] - 8s 183ms/step - loss: 0.0354 - val_loss: 0.4321\n",
            "Epoch 172/200\n",
            "46/46 [==============================] - 8s 179ms/step - loss: 0.0354 - val_loss: 0.4459\n",
            "Epoch 173/200\n",
            "46/46 [==============================] - 8s 181ms/step - loss: 0.0355 - val_loss: 0.4494\n",
            "Epoch 174/200\n",
            "46/46 [==============================] - 8s 181ms/step - loss: 0.0354 - val_loss: 0.4362\n",
            "Epoch 175/200\n",
            "46/46 [==============================] - 8s 179ms/step - loss: 0.0349 - val_loss: 0.4553\n",
            "Epoch 176/200\n",
            "46/46 [==============================] - 8s 178ms/step - loss: 0.0348 - val_loss: 0.4447\n",
            "Epoch 177/200\n",
            "46/46 [==============================] - 8s 180ms/step - loss: 0.0343 - val_loss: 0.4439\n",
            "Epoch 178/200\n",
            "46/46 [==============================] - 8s 181ms/step - loss: 0.0344 - val_loss: 0.4504\n",
            "Epoch 179/200\n",
            "46/46 [==============================] - 8s 179ms/step - loss: 0.0345 - val_loss: 0.4562\n",
            "Epoch 180/200\n",
            "46/46 [==============================] - 8s 182ms/step - loss: 0.0344 - val_loss: 0.4465\n",
            "Epoch 181/200\n",
            "46/46 [==============================] - 8s 180ms/step - loss: 0.0344 - val_loss: 0.4670\n",
            "Epoch 182/200\n",
            "46/46 [==============================] - 8s 180ms/step - loss: 0.0347 - val_loss: 0.4550\n",
            "Epoch 183/200\n",
            "46/46 [==============================] - 8s 181ms/step - loss: 0.0340 - val_loss: 0.4541\n",
            "Epoch 184/200\n",
            "46/46 [==============================] - 8s 177ms/step - loss: 0.0341 - val_loss: 0.4545\n",
            "Epoch 185/200\n",
            "46/46 [==============================] - 8s 176ms/step - loss: 0.0338 - val_loss: 0.4500\n",
            "Epoch 186/200\n",
            "46/46 [==============================] - 8s 179ms/step - loss: 0.0334 - val_loss: 0.4601\n",
            "Epoch 187/200\n",
            "46/46 [==============================] - 8s 178ms/step - loss: 0.0333 - val_loss: 0.4592\n",
            "Epoch 188/200\n",
            "46/46 [==============================] - 8s 179ms/step - loss: 0.0332 - val_loss: 0.4607\n",
            "Epoch 189/200\n",
            "46/46 [==============================] - 8s 179ms/step - loss: 0.0331 - val_loss: 0.4697\n",
            "Epoch 190/200\n",
            "46/46 [==============================] - 8s 180ms/step - loss: 0.0332 - val_loss: 0.4668\n",
            "Epoch 191/200\n",
            "46/46 [==============================] - 8s 178ms/step - loss: 0.0329 - val_loss: 0.4697\n",
            "Epoch 192/200\n",
            "46/46 [==============================] - 8s 181ms/step - loss: 0.0328 - val_loss: 0.4575\n",
            "Epoch 193/200\n",
            "46/46 [==============================] - 8s 178ms/step - loss: 0.0328 - val_loss: 0.4641\n",
            "Epoch 194/200\n",
            "46/46 [==============================] - 8s 182ms/step - loss: 0.0326 - val_loss: 0.4654\n",
            "Epoch 195/200\n",
            "46/46 [==============================] - 8s 184ms/step - loss: 0.0327 - val_loss: 0.4661\n",
            "Epoch 196/200\n",
            "46/46 [==============================] - 8s 183ms/step - loss: 0.0328 - val_loss: 0.4639\n",
            "Epoch 197/200\n",
            "46/46 [==============================] - 8s 180ms/step - loss: 0.0328 - val_loss: 0.4711\n",
            "Epoch 198/200\n",
            "46/46 [==============================] - 8s 180ms/step - loss: 0.0327 - val_loss: 0.4648\n",
            "Epoch 199/200\n",
            "46/46 [==============================] - 8s 181ms/step - loss: 0.0324 - val_loss: 0.4629\n",
            "Epoch 200/200\n",
            "46/46 [==============================] - 8s 180ms/step - loss: 0.0320 - val_loss: 0.4653\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7ff264480d90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8h_rUAeM5AdB"
      },
      "source": [
        "### 3) 번역기 동작"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yiADr6fy5B06"
      },
      "source": [
        "encoder_model = Model(inputs = encoder_inputs,\n",
        "                      outputs = encoder_states)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4HLm9tm5Yrt"
      },
      "source": [
        "# 이전 시점 상태들을 저장하는 텐서\n",
        "decoder_state_input_h = Input(shape = (256,))\n",
        "decoder_state_input_c = Input(shape = (256,))\n",
        "decoder_states_inputs = [decoder_state_input_h , decoder_state_input_c]\n",
        "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs,\n",
        "                                                 initial_state = decoder_states_inputs)\n",
        "# 다음 단어를 예측하기 위해 초기상태를 이전 시점의 상태로 사용\n",
        "# 이는 뒤 함수 decode_sequence()에 구현\n",
        "\n",
        "decoder_states = [state_h, state_c]\n",
        "\n",
        "decoder_outputs = decoder_softmax_layer(decoder_outputs)\n",
        "decoder_model = Model(inputs = [decoder_inputs] + decoder_states_inputs,\n",
        "                      outputs = [decoder_outputs] + decoder_states)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fip4RFYk6VF-"
      },
      "source": [
        "index_to_src = dict((i, char) for char, i in src_to_index.items())\n",
        "index_to_tar = dict((i, char) for char, i in tar_to_index.items())"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rm1VCQb-6eQz"
      },
      "source": [
        "def decode_sequence(input_seq) :\n",
        "\n",
        "  #입력으로부터 인코더의 상태를 얻음\n",
        "  states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "  # <SOS>에 해당하는 원핫벡터 생성\n",
        "  target_seq = np.zeros((1, 1, tar_vocab_size))\n",
        "  target_seq[0, 0, tar_to_index['\\t']] = 1.\n",
        "\n",
        "  stop_condition = False\n",
        "  decoded_sentence = ''\n",
        "\n",
        "  # stop_condition이 True가 될 때까지 반복\n",
        "  while not stop_condition:\n",
        "\n",
        "    # 이전 시점의 상태 states_value를 현 시점의 초기 상태로 사용\n",
        "    output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "\n",
        "    # 예측 결과를 문자로 변환\n",
        "    sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "    sampled_char = index_to_tar[sampled_token_index]\n",
        "\n",
        "    # 현재 시점의 예측문자를 예측문장에 추가\n",
        "    decoded_sentence += sampled_char\n",
        "\n",
        "    # <eos>에 도달하거나 최대 길이를 넘으면 중단\n",
        "    if (sampled_char == '\\n' or len(decoded_sentence) > max_tar_len) :\n",
        "        stop_condition = True\n",
        "\n",
        "    # 현재 시점의 예측 결과를 다음 시점의 입력으로 사용하기 위해 저장\n",
        "    target_seq = np.zeros((1, 1, tar_vocab_size))\n",
        "    target_seq[0, 0, sampled_token_index] = 1.\n",
        "\n",
        "    # 현재 시점의 상태를 다음 시점의 상태로 사용하기 위해 저장\n",
        "    states_value = [h, c]\n",
        "\n",
        "  return decoded_sentence"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NThfEyz-7vkr",
        "outputId": "904decea-087b-4687-9217-24abd258b5fc"
      },
      "source": [
        "for i in range(10) :\n",
        "  seq_index = random.randint(10, 300)\n",
        "  input_seq = encoder_input[seq_index:seq_index + 1]\n",
        "  decoded_sentence = decode_sequence(input_seq)\n",
        "\n",
        "  print('-' * 35)\n",
        "  print('입력문장: ' , lines.src[seq_index])\n",
        "  print('정답문장: ' , lines.tar[seq_index][1:len(lines.tar[seq_index]) - 1])\n",
        "  # '\\t' , '\\n' 빼고 출력\n",
        "  print('번역기가 번역한 문장: ' , decoded_sentence[:len(decoded_sentence) -1])\n",
        "  # '\\n'을 빼고 출력"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-----------------------------------\n",
            "입력문장:  Is Tom ill?\n",
            "정답문장:  톰은 아파?\n",
            "번역기가 번역한 문장:  톰은 자기 컴퓨터를 껐어.\n",
            "-----------------------------------\n",
            "입력문장:  Is that OK?\n",
            "정답문장:  괜찮은 거예요?\n",
            "번역기가 번역한 문장:  톰은 자기 컴퓨터를 껐어.\n",
            "-----------------------------------\n",
            "입력문장:  Skip it.\n",
            "정답문장:  건너뛰어.\n",
            "번역기가 번역한 문장:  톰은 자기 컴퓨터를 껐어.\n",
            "-----------------------------------\n",
            "입력문장:  What fun!\n",
            "정답문장:  재밌잖아!\n",
            "번역기가 번역한 문장:  톰은 자기 컴퓨터를 껐어.\n",
            "-----------------------------------\n",
            "입력문장:  Tom won.\n",
            "정답문장:  톰이 이겼어.\n",
            "번역기가 번역한 문장:  톰은 자기 컴퓨터를 껐어.\n",
            "-----------------------------------\n",
            "입력문장:  Who died?\n",
            "정답문장:  누가 죽었어?\n",
            "번역기가 번역한 문장:  톰은 자기 컴퓨터를 껐어.\n",
            "-----------------------------------\n",
            "입력문장:  Forget Tom.\n",
            "정답문장:  톰은 잊어버려.\n",
            "번역기가 번역한 문장:  톰은 자기 컴퓨터를 껐어.\n",
            "-----------------------------------\n",
            "입력문장:  Take this.\n",
            "정답문장:  이걸 가져.\n",
            "번역기가 번역한 문장:  톰은 자기 컴퓨터를 껐어.\n",
            "-----------------------------------\n",
            "입력문장:  It works.\n",
            "정답문장:  되네.\n",
            "번역기가 번역한 문장:  톰은 자기 컴퓨터를 껐어.\n",
            "-----------------------------------\n",
            "입력문장:  Come aboard.\n",
            "정답문장:  외국으로 와.\n",
            "번역기가 번역한 문장:  톰은 자기 컴퓨터를 껐어.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWFzJaE0x0fQ"
      },
      "source": [
        "## 2. 단어 단위 번역기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XX0ImSAix8C_"
      },
      "source": [
        "### 1) 데이터 전처리\n",
        "- 전처리 함수 구현"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqlOL3HaFkhM"
      },
      "source": [
        "def unicode_to_ascii(s) :\n",
        "  return ''.join(c for c in unicodedata.normalize('NFD' , s)\n",
        "  if unicodedata.category(c) != 'Mn')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBzKkNItyvfB"
      },
      "source": [
        "def preprocess_sentence(sent) :\n",
        "  # 위에서 구현한 함수를 내부적으로 호출\n",
        "  # sent = unicode_to_ascii(sent.lower())\n",
        "\n",
        "  # 단어와 구두점 사이에 공백 만듦\n",
        "  sent = re.sub(r'([?.!,¿])' , r' \\1' , sent)\n",
        "\n",
        "  # (a-z, A-Z, '.', '?', '!', ',')를 제외하고 전부 공백으로 변환\n",
        "  sent = re.sub(r'[^a-zA-Z!.?ㄱ-하-ㅣ가-힣]+' , r' ' , sent)\n",
        "\n",
        "  # sent = re.sub(r'\\s+' , ' ' , sent)\n",
        "  return sent"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dwg-Vcy8zhyI"
      },
      "source": [
        "- 전처리 테스트"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Te86je1ZzhLd",
        "outputId": "76f1165b-4668-41e0-e2a1-5d59463303b0"
      },
      "source": [
        "en_sent = u'Have you had dinner?'\n",
        "ko_sent = u'밥 먹었니?'\n",
        "\n",
        "print(preprocess_sentence(en_sent))\n",
        "print(preprocess_sentence(ko_sent))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Have you had dinner ?\n",
            "밥 먹었니 ?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dro9kWZo0dPz"
      },
      "source": [
        "def load_preprocessed_data():\n",
        "  encoder_input, decoder_input, decoder_target = [], [], []\n",
        "\n",
        "  with open('kor.txt' , 'r' , encoding = 'utf-8') as lines:\n",
        "    for i, line in enumerate(lines):\n",
        "\n",
        "      # source 데이터와 target 데이터 분리\n",
        "      src_line, tar_line, _ = line.strip().split('\\t')\n",
        "\n",
        "      # source 데이터 전처리\n",
        "      src_line_input = [ w for w in preprocess_sentence(src_line).split()]\n",
        "\n",
        "      # target 데이터 전처리\n",
        "      tar_line = preprocess_sentence(tar_line)\n",
        "      tar_line_input = [ w for w in (\"<sos> \" + tar_line).split()]\n",
        "      tar_line_target = [ w for w in (tar_line + \" <sos>\").split()]\n",
        "\n",
        "      encoder_input.append(src_line_input)\n",
        "      decoder_input.append(tar_line_input)\n",
        "      decoder_target.append(tar_line_target)\n",
        "\n",
        "\n",
        "  return encoder_input, decoder_input, decoder_target"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eV194ZHH23FB",
        "outputId": "43fc16cb-4d17-4bb0-96ce-1fee9dc2e3cb"
      },
      "source": [
        "sents_en_in, sents_ko_in, sents_ko_out = load_preprocessed_data()\n",
        "\n",
        "print(sents_en_in[:5])\n",
        "print(sents_ko_in[:5])\n",
        "print(sents_ko_out[:5])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['Go', '.'], ['Hi', '.'], ['Run', '!'], ['Run', '.'], ['Who', '?']]\n",
            "[['<sos>', '가', '.'], ['<sos>', '안녕', '.'], ['<sos>', '뛰어', '!'], ['<sos>', '뛰어', '.'], ['<sos>', '누구', '?']]\n",
            "[['가', '.', '<sos>'], ['안녕', '.', '<sos>'], ['뛰어', '!', '<sos>'], ['뛰어', '.', '<sos>'], ['누구', '?', '<sos>']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzU5dZuc6OMs"
      },
      "source": [
        "- 단어집합 생성, sequence-to-sequence로 변환하는 정수 인코딩"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKCspud93HZG"
      },
      "source": [
        "tokenizer_en = Tokenizer(filters = '', lower = False)\n",
        "tokenizer_en.fit_on_texts(sents_en_in)\n",
        "encoder_input = tokenizer_en.texts_to_sequences(sents_en_in)\n",
        "\n",
        "tokenizer_ko = Tokenizer(filters = '', lower = False)\n",
        "tokenizer_ko.fit_on_texts(sents_ko_in)\n",
        "tokenizer_ko.fit_on_texts(sents_ko_out)\n",
        "decoder_input = tokenizer_ko.texts_to_sequences(sents_ko_in)\n",
        "decoder_target = tokenizer_ko.texts_to_sequences(sents_ko_out)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJbGQ9dT6UAa"
      },
      "source": [
        "- 패딩"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8SHLh8km6Gsl"
      },
      "source": [
        "encoder_input = pad_sequences(encoder_input, padding = 'post')\n",
        "decoder_input = pad_sequences(decoder_input, padding = 'post')\n",
        "decoder_target = pad_sequences(decoder_target, padding = 'post')"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ggXbDYms6jD1",
        "outputId": "e985b623-de3f-49d6-b779-ce6cf2c70c79"
      },
      "source": [
        "src_vocab_size = len(tokenizer_en.word_index) + 1\n",
        "tar_vocab_size = len(tokenizer_ko.word_index) + 1\n",
        "\n",
        "print('영어 단어 집합의 크기: {:d}, 한국어 단어 집합의 크기: {:d}' .format(src_vocab_size, tar_vocab_size))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "영어 단어 집합의 크기: 2751, 한국어 단어 집합의 크기: 5520\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJ7Hk7IQ64EZ"
      },
      "source": [
        "- 단어 -> 정수, 정수 -> 단어 딕셔너리 생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvTMnKxG60G7"
      },
      "source": [
        "src_to_index = tokenizer_en.word_index\n",
        "index_to_src = tokenizer_en.index_word # 훈련 후 결과 비교할 때 사용\n",
        "\n",
        "tar_to_index = tokenizer_ko.word_index # 훈련 후 예측 과정에서 사용\n",
        "index_to_tar = tokenizer_ko.index_word # 훈련 후 결과 비교할 때 사용"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCWLLVpR7VMx"
      },
      "source": [
        "- 적절한 분포를 갖도록 데이터를 섞어줌"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TqEzIQ997Txr",
        "outputId": "61fc0855-8861-4e72-f04e-22e6c14f78d7"
      },
      "source": [
        "encoder_input.shape[0]"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3648"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "flvEUFEK7dkQ",
        "outputId": "7fe0ba96-8aed-43d7-e909-b11223dcfa9a"
      },
      "source": [
        "indices = np.arange(encoder_input.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "\n",
        "indices"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2712, 2129,  610, ...,  928,  149, 1200])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4Pxu2WP7ljG"
      },
      "source": [
        "# indices를 데이터셋의 순서로 지정해주면 샘플들이 기존순서와 다른 순서로 섞임\n",
        "encoder_input = encoder_input[indices]\n",
        "decoder_input = decoder_input[indices]\n",
        "decoder_target = decoder_target[indices]"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sU4d9AGL72n2",
        "outputId": "1771d8e6-7137-4bb1-9b9b-5ab7dbb5f4d6"
      },
      "source": [
        "encoder_input[2000] , decoder_input[2000] , decoder_target[2000]"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([  34,   11,   58, 1666,    1,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0], dtype=int32),\n",
              " array([   1,   34,  142, 2405,    2,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0], dtype=int32),\n",
              " array([  34,  142, 2405,    2,    1,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0], dtype=int32))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OH_rcqr-8GpK"
      },
      "source": [
        "- 훈련데이터의 10%를 테스트 데이터로 분리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_rWYeHu_78XG",
        "outputId": "88557787-f0cb-4fbd-c193-b7e76b84da53"
      },
      "source": [
        "n_of_val = encoder_input.shape[0] // 10\n",
        "\n",
        "n_of_val"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "364"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCTlomq58SkD"
      },
      "source": [
        "encoder_input_train = encoder_input[:-n_of_val]\n",
        "decoder_input_train = decoder_input[:-n_of_val]\n",
        "decoder_target_train = decoder_target[:-n_of_val]\n",
        "\n",
        "encoder_input_test = encoder_input[-n_of_val:]\n",
        "decoder_input_test = decoder_input[-n_of_val:]\n",
        "decoder_target_test = decoder_target[-n_of_val:]"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oj0FIDzy8t8U",
        "outputId": "be0d19b5-7bd1-4ba0-fe76-29b4d77dd61f"
      },
      "source": [
        "encoder_input_train.shape, decoder_input_train.shape, decoder_target_train.shape"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((3284, 102), (3284, 92), (3284, 92))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ELRmLWS81hI",
        "outputId": "8f5d0f72-25bd-4e70-cf86-fe344604a16c"
      },
      "source": [
        "encoder_input_test.shape, decoder_input_test.shape, decoder_target_test.shape"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((364, 102), (364, 92), (364, 92))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EdfXSHp9hvM"
      },
      "source": [
        "### 2) 기계번역기 만들기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8znFn6P9plU"
      },
      "source": [
        "- 임베딩 벡터와 LSTM 은닉상태의 크기를 50으로 고정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aaEiLcAd9KTU"
      },
      "source": [
        "latent_dim = 50"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfnJ4Fqw9u8y"
      },
      "source": [
        "- 인코더 설계"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7I-eNYR9tzH"
      },
      "source": [
        "encoder_inputs = Input(shape = (None,))\n",
        "enc_emb = Embedding(src_vocab_size, latent_dim)(encoder_inputs)  # 임베딩 층\n",
        "enc_masking = Masking(mask_value = 0.0)(enc_emb)  # 패딩 0은 연산에서 제외\n",
        "encoder_lstm = LSTM(latent_dim, return_state = True)  # 상태값 리턴을 위해 return_state = True\n",
        "encoder_outputs, state_h, state_c = encoder_lstm(enc_masking) # 은닉상태와 셀 상태를 리턴\n",
        "encoder_states = [state_h, state_c]  # 인코더의 은닉 상태와 셀 상태를 저장"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKbmS7q7-TJr"
      },
      "source": [
        "- 디코더 설계"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XcQEftTc-R7t"
      },
      "source": [
        "decoder_inputs = Input(shape = (None,))\n",
        "dec_emb_layer = Embedding(tar_vocab_size, latent_dim) # 임베딩 층\n",
        "dec_emb = dec_emb_layer(decoder_inputs) # 패딩 0은 연산에서 제외\n",
        "dec_masking = Masking(mask_value = 0.0)(dec_emb)\n",
        "\n",
        "decoder_lstm = LSTM(latent_dim,\n",
        "                    return_sequences = True,  # 모든 시점에 대해서 단어를 예측하기 위해\n",
        "                    return_state = True)      # 상태값 리턴을 위해\n",
        "\n",
        "decoder_outputs, _, _ = decoder_lstm(dec_masking,\n",
        "                                     initial_state = encoder_states)\n",
        "# 인코더의 은닉 상태를 초기 은닉상태로 사용\n",
        "\n",
        "decoder_dense = Dense(tar_vocab_size, activation = 'softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "# 모든 시점의 결과에 대해 소프트맥스 함수를 사용해 단어 예측"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LObw4FHW_cqZ"
      },
      "source": [
        "model = Model([encoder_inputs, decoder_inputs] , decoder_outputs)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40lAZJJyB0b6",
        "outputId": "2820a397-9fc6-42b7-a828-4b08d1fc4d05"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_3 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_4 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_2 (Embedding)         (None, None, 50)     137550      input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_3 (Embedding)         (None, None, 50)     276000      input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "masking_2 (Masking)             (None, None, 50)     0           embedding_2[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "masking_3 (Masking)             (None, None, 50)     0           embedding_3[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "lstm_2 (LSTM)                   [(None, 50), (None,  20200       masking_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lstm_3 (LSTM)                   [(None, None, 50), ( 20200       masking_3[0][0]                  \n",
            "                                                                 lstm_2[0][1]                     \n",
            "                                                                 lstm_2[0][2]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, None, 5520)   281520      lstm_3[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 735,470\n",
            "Trainable params: 735,470\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KD4CXvoMB1Lt"
      },
      "source": [
        "# 원핫인코딩하지 않은 상태로 다중클래스 분류를 하기 위해서 sparse_categorical_crossentropy 사용\n",
        "model.compile(loss = 'sparse_categorical_crossentropy',\n",
        "              optimizer = 'rmsprop',\n",
        "              metrics = ['accuracy'])"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6dxv0n4mCY-l",
        "outputId": "ed6936d5-bfda-45c1-c282-36295f448cf2"
      },
      "source": [
        "%%time\n",
        "\n",
        "hist = model.fit(x = [encoder_input_train, decoder_input_train],\n",
        "                 y = decoder_target_train,\n",
        "                 validation_data = ([encoder_input_test, decoder_input_test],\n",
        "                                    decoder_target_test),\n",
        "                 batch_size = 128,\n",
        "                 epochs = 50)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "26/26 [==============================] - 16s 136ms/step - loss: 7.6651 - accuracy: 0.7983 - val_loss: 4.5591 - val_accuracy: 0.9322\n",
            "Epoch 2/50\n",
            "26/26 [==============================] - 1s 49ms/step - loss: 3.8332 - accuracy: 0.9332 - val_loss: 1.9784 - val_accuracy: 0.9322\n",
            "Epoch 3/50\n",
            "26/26 [==============================] - 1s 49ms/step - loss: 1.5984 - accuracy: 0.9323 - val_loss: 0.8533 - val_accuracy: 0.9322\n",
            "Epoch 4/50\n",
            "26/26 [==============================] - 1s 50ms/step - loss: 0.7547 - accuracy: 0.9328 - val_loss: 0.5792 - val_accuracy: 0.9322\n",
            "Epoch 5/50\n",
            "26/26 [==============================] - 1s 50ms/step - loss: 0.5535 - accuracy: 0.9319 - val_loss: 0.4899 - val_accuracy: 0.9322\n",
            "Epoch 6/50\n",
            "26/26 [==============================] - 1s 51ms/step - loss: 0.4840 - accuracy: 0.9313 - val_loss: 0.4500 - val_accuracy: 0.9322\n",
            "Epoch 7/50\n",
            "26/26 [==============================] - 1s 50ms/step - loss: 0.4313 - accuracy: 0.9326 - val_loss: 0.4305 - val_accuracy: 0.9325\n",
            "Epoch 8/50\n",
            "26/26 [==============================] - 1s 50ms/step - loss: 0.4138 - accuracy: 0.9331 - val_loss: 0.4183 - val_accuracy: 0.9360\n",
            "Epoch 9/50\n",
            "26/26 [==============================] - 1s 49ms/step - loss: 0.3897 - accuracy: 0.9406 - val_loss: 0.4127 - val_accuracy: 0.9441\n",
            "Epoch 10/50\n",
            "26/26 [==============================] - 1s 49ms/step - loss: 0.3917 - accuracy: 0.9428 - val_loss: 0.4085 - val_accuracy: 0.9443\n",
            "Epoch 11/50\n",
            "26/26 [==============================] - 1s 49ms/step - loss: 0.3762 - accuracy: 0.9447 - val_loss: 0.4069 - val_accuracy: 0.9475\n",
            "Epoch 12/50\n",
            "26/26 [==============================] - 1s 49ms/step - loss: 0.3680 - accuracy: 0.9478 - val_loss: 0.4036 - val_accuracy: 0.9507\n",
            "Epoch 13/50\n",
            "26/26 [==============================] - 1s 49ms/step - loss: 0.3672 - accuracy: 0.9498 - val_loss: 0.4019 - val_accuracy: 0.9528\n",
            "Epoch 14/50\n",
            "26/26 [==============================] - 1s 49ms/step - loss: 0.3603 - accuracy: 0.9526 - val_loss: 0.4020 - val_accuracy: 0.9534\n",
            "Epoch 15/50\n",
            "26/26 [==============================] - 1s 50ms/step - loss: 0.3539 - accuracy: 0.9534 - val_loss: 0.3994 - val_accuracy: 0.9535\n",
            "Epoch 16/50\n",
            "26/26 [==============================] - 1s 50ms/step - loss: 0.3478 - accuracy: 0.9537 - val_loss: 0.3973 - val_accuracy: 0.9535\n",
            "Epoch 17/50\n",
            "26/26 [==============================] - 1s 49ms/step - loss: 0.3469 - accuracy: 0.9535 - val_loss: 0.3974 - val_accuracy: 0.9534\n",
            "Epoch 18/50\n",
            "26/26 [==============================] - 1s 49ms/step - loss: 0.3427 - accuracy: 0.9534 - val_loss: 0.3970 - val_accuracy: 0.9535\n",
            "Epoch 19/50\n",
            "26/26 [==============================] - 1s 49ms/step - loss: 0.3465 - accuracy: 0.9525 - val_loss: 0.3959 - val_accuracy: 0.9535\n",
            "Epoch 20/50\n",
            "26/26 [==============================] - 1s 49ms/step - loss: 0.3366 - accuracy: 0.9535 - val_loss: 0.3972 - val_accuracy: 0.9534\n",
            "Epoch 21/50\n",
            "26/26 [==============================] - 1s 49ms/step - loss: 0.3343 - accuracy: 0.9535 - val_loss: 0.3966 - val_accuracy: 0.9535\n",
            "Epoch 22/50\n",
            "26/26 [==============================] - 1s 49ms/step - loss: 0.3322 - accuracy: 0.9535 - val_loss: 0.3981 - val_accuracy: 0.9535\n",
            "Epoch 23/50\n",
            "26/26 [==============================] - 1s 49ms/step - loss: 0.3292 - accuracy: 0.9538 - val_loss: 0.3976 - val_accuracy: 0.9537\n",
            "Epoch 24/50\n",
            "26/26 [==============================] - 1s 51ms/step - loss: 0.3281 - accuracy: 0.9537 - val_loss: 0.3980 - val_accuracy: 0.9537\n",
            "Epoch 25/50\n",
            "26/26 [==============================] - 1s 50ms/step - loss: 0.3323 - accuracy: 0.9529 - val_loss: 0.3987 - val_accuracy: 0.9537\n",
            "Epoch 26/50\n",
            "26/26 [==============================] - 1s 50ms/step - loss: 0.3260 - accuracy: 0.9537 - val_loss: 0.3999 - val_accuracy: 0.9538\n",
            "Epoch 27/50\n",
            "26/26 [==============================] - 1s 49ms/step - loss: 0.3196 - accuracy: 0.9544 - val_loss: 0.4007 - val_accuracy: 0.9538\n",
            "Epoch 28/50\n",
            "26/26 [==============================] - 1s 50ms/step - loss: 0.3194 - accuracy: 0.9543 - val_loss: 0.4007 - val_accuracy: 0.9537\n",
            "Epoch 29/50\n",
            "26/26 [==============================] - 1s 49ms/step - loss: 0.3279 - accuracy: 0.9533 - val_loss: 0.4015 - val_accuracy: 0.9532\n",
            "Epoch 30/50\n",
            "26/26 [==============================] - 1s 50ms/step - loss: 0.3263 - accuracy: 0.9545 - val_loss: 0.4027 - val_accuracy: 0.9528\n",
            "Epoch 31/50\n",
            "26/26 [==============================] - 1s 49ms/step - loss: 0.3157 - accuracy: 0.9544 - val_loss: 0.4017 - val_accuracy: 0.9524\n",
            "Epoch 32/50\n",
            "26/26 [==============================] - 1s 50ms/step - loss: 0.3198 - accuracy: 0.9539 - val_loss: 0.4016 - val_accuracy: 0.9522\n",
            "Epoch 33/50\n",
            "26/26 [==============================] - 1s 50ms/step - loss: 0.3156 - accuracy: 0.9545 - val_loss: 0.4007 - val_accuracy: 0.9521\n",
            "Epoch 34/50\n",
            "26/26 [==============================] - 1s 50ms/step - loss: 0.3190 - accuracy: 0.9541 - val_loss: 0.4005 - val_accuracy: 0.9519\n",
            "Epoch 35/50\n",
            "26/26 [==============================] - 1s 51ms/step - loss: 0.3123 - accuracy: 0.9544 - val_loss: 0.4006 - val_accuracy: 0.9518\n",
            "Epoch 36/50\n",
            "26/26 [==============================] - 1s 50ms/step - loss: 0.3159 - accuracy: 0.9539 - val_loss: 0.4026 - val_accuracy: 0.9519\n",
            "Epoch 37/50\n",
            "26/26 [==============================] - 1s 50ms/step - loss: 0.3115 - accuracy: 0.9545 - val_loss: 0.4011 - val_accuracy: 0.9517\n",
            "Epoch 38/50\n",
            "26/26 [==============================] - 1s 50ms/step - loss: 0.3103 - accuracy: 0.9544 - val_loss: 0.4028 - val_accuracy: 0.9519\n",
            "Epoch 39/50\n",
            "26/26 [==============================] - 1s 49ms/step - loss: 0.3025 - accuracy: 0.9553 - val_loss: 0.4050 - val_accuracy: 0.9519\n",
            "Epoch 40/50\n",
            "26/26 [==============================] - 1s 50ms/step - loss: 0.3110 - accuracy: 0.9541 - val_loss: 0.4044 - val_accuracy: 0.9520\n",
            "Epoch 41/50\n",
            "26/26 [==============================] - 1s 51ms/step - loss: 0.3035 - accuracy: 0.9548 - val_loss: 0.4049 - val_accuracy: 0.9520\n",
            "Epoch 42/50\n",
            "26/26 [==============================] - 1s 51ms/step - loss: 0.3028 - accuracy: 0.9549 - val_loss: 0.4044 - val_accuracy: 0.9519\n",
            "Epoch 43/50\n",
            "26/26 [==============================] - 1s 50ms/step - loss: 0.3019 - accuracy: 0.9548 - val_loss: 0.4057 - val_accuracy: 0.9520\n",
            "Epoch 44/50\n",
            "26/26 [==============================] - 1s 50ms/step - loss: 0.2990 - accuracy: 0.9551 - val_loss: 0.4068 - val_accuracy: 0.9520\n",
            "Epoch 45/50\n",
            "26/26 [==============================] - 1s 50ms/step - loss: 0.2983 - accuracy: 0.9553 - val_loss: 0.4087 - val_accuracy: 0.9522\n",
            "Epoch 46/50\n",
            "26/26 [==============================] - 1s 50ms/step - loss: 0.2927 - accuracy: 0.9560 - val_loss: 0.4085 - val_accuracy: 0.9522\n",
            "Epoch 47/50\n",
            "26/26 [==============================] - 1s 50ms/step - loss: 0.3036 - accuracy: 0.9544 - val_loss: 0.4069 - val_accuracy: 0.9519\n",
            "Epoch 48/50\n",
            "26/26 [==============================] - 1s 50ms/step - loss: 0.3017 - accuracy: 0.9548 - val_loss: 0.4056 - val_accuracy: 0.9520\n",
            "Epoch 49/50\n",
            "26/26 [==============================] - 1s 51ms/step - loss: 0.2910 - accuracy: 0.9558 - val_loss: 0.4024 - val_accuracy: 0.9521\n",
            "Epoch 50/50\n",
            "26/26 [==============================] - 1s 50ms/step - loss: 0.2914 - accuracy: 0.9557 - val_loss: 0.4039 - val_accuracy: 0.9521\n",
            "CPU times: user 46.5 s, sys: 5.23 s, total: 51.7 s\n",
            "Wall time: 1min 19s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYXYNGLgDC--"
      },
      "source": [
        "### 3) 기계번역기 동작시키기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVeceSfoDHJ8"
      },
      "source": [
        "- 인코더 설계"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gowiZk_YCupU"
      },
      "source": [
        "encoder_model = Model(encoder_inputs, encoder_states)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0beUeNRrDNhE",
        "outputId": "68a15c9e-b124-452a-cb7c-349dc569463c"
      },
      "source": [
        "encoder_model.summary()"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_3 (InputLayer)         [(None, None)]            0         \n",
            "_________________________________________________________________\n",
            "embedding_2 (Embedding)      (None, None, 50)          137550    \n",
            "_________________________________________________________________\n",
            "masking_2 (Masking)          (None, None, 50)          0         \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                [(None, 50), (None, 50),  20200     \n",
            "=================================================================\n",
            "Total params: 157,750\n",
            "Trainable params: 157,750\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZX8o2nMDQ8U"
      },
      "source": [
        "- 디코더 설계"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7kFyJaIDDOt0"
      },
      "source": [
        "# 이전 시점의 상태를 보관할 텐서\n",
        "decoder_state_input_h = Input(shape = (latent_dim, ))\n",
        "decoder_state_input_c = Input(shape = (latent_dim, ))\n",
        "decoder_states_inputs = [decoder_state_input_h , decoder_state_input_c]\n",
        "\n",
        "# 훈련 때 사용했던 임베딩 층 재사용\n",
        "dec_emb2 = dec_emb_layer(decoder_inputs)\n",
        "\n",
        "# 다음 단어 예측을 위해 예전 시점의 상태를 현 시점의 초기 상태로 사용\n",
        "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2,\n",
        "                                                    initial_state = decoder_states_inputs)\n",
        "decoder_states2 = [state_h2, state_c2]\n",
        "\n",
        "# 모든 시점에 대해서 단어 예측\n",
        "decoder_outputs2 = decoder_dense(decoder_outputs2)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HF187ET9D6NW"
      },
      "source": [
        "- 디코더 정의"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tb2lR3FjD5us"
      },
      "source": [
        "decoder_model = Model(\n",
        "    [decoder_inputs] + decoder_states_inputs,\n",
        "    [decoder_outputs2] + decoder_states2)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ROfvT1cVEEEO",
        "outputId": "f15ec9f6-2e15-4b99-9215-e41995036851"
      },
      "source": [
        "decoder_model.summary()"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_4 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_3 (Embedding)         (None, None, 50)     276000      input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "input_5 (InputLayer)            [(None, 50)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_6 (InputLayer)            [(None, 50)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm_3 (LSTM)                   [(None, None, 50), ( 20200       embedding_3[1][0]                \n",
            "                                                                 input_5[0][0]                    \n",
            "                                                                 input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, None, 5520)   281520      lstm_3[1][0]                     \n",
            "==================================================================================================\n",
            "Total params: 577,720\n",
            "Trainable params: 577,720\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-A0pJfXEKin"
      },
      "source": [
        "- 동작을 위한 decode_sequence 함수 구현"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74EovO02EFAS"
      },
      "source": [
        "def decode_sequence(input_seq) :\n",
        "\n",
        "  # 입력으로부터 인코더의 상태를 얻음\n",
        "  states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "  # <sos>에 해당하는 정수 생성\n",
        "  target_seq = np.zeros((1,1))\n",
        "  target_seq[0, 0] = tar_to_index['<sos>']\n",
        "\n",
        "  stop_condition = False\n",
        "  decoded_sentence = ''\n",
        "\n",
        "  # stop_condition이 True가 될 때까지 루ㅠㅡ 반복\n",
        "\n",
        "  while not stop_condition:\n",
        "    output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "\n",
        "    # 에측 결과를 단어로 변환\n",
        "    sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "    sampled_char = index_to_tar[sampled_token_index]\n",
        "\n",
        "    # 현재 시점의 예측 단어를 예측 문장에 추가\n",
        "    decoded_sentence += ' '+sampled_char\n",
        "\n",
        "    # <eos>에 도달하거나 정해진 길이를 넘으면 중단\n",
        "    if (sampled_char == '<eos>' or len(decoded_sentence) > 50):\n",
        "      stop_condition = True\n",
        "    \n",
        "    # 현재 시점의 예측 결과를 다음 시점의 입력으로 사용하기 위해 저장\n",
        "    target_seq = np.zeros((1, 1))\n",
        "    target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "    # 현재 시점의 상태를 다음 시점의 상태로 사용하기 위해 저장\n",
        "    states_value = [h, c]\n",
        "\n",
        "  return decoded_sentence"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LbEQCcKFVoW"
      },
      "source": [
        "- 결과 확인을 위한 함수 생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t672iR8YFPZh"
      },
      "source": [
        "# 원문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
        "def seq2seq(input_seq):\n",
        "  temp = ''\n",
        "  for i in input_seq:\n",
        "    if (i != 0):\n",
        "      temp = temp + index_to_src[i] + ' '\n",
        "  return temp"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6fzbbL-Fqym"
      },
      "source": [
        "# 번역문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
        "def seq2tar(input_seq):\n",
        "  temp = ''\n",
        "  for i in input_seq:\n",
        "    if ((i!=0 and i!=tar_to_index['<sos>'])):\n",
        "      temp = temp + index_to_tar[i] + ' '\n",
        "  return temp"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_zEoNPfF_TX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NvWkw1iGBWq"
      },
      "source": [
        "- 임의로 선택한 인덱스의 샘플 결과를 출력"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_pFsDbd5GC-R",
        "outputId": "f4fd7807-4a5b-4561-83cd-1c6afc69af4e"
      },
      "source": [
        "for seq_index in [3, 50, 100, 300, 1001] :\n",
        "  input_seq = encoder_input_train[seq_index: seq_index + 1]\n",
        "  decoded_sentence = decode_sequence(input_seq)\n",
        "\n",
        "\n",
        "  print(\"원문: \" , seq2seq(encoder_input_train[seq_index]))\n",
        "  print(\"번역문: \" , seq2tar(decoder_input_train[seq_index]))\n",
        "  print(\"예측문: \" , decoded_sentence[:-5])\n",
        "  print('\\n')"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "원문:  Her dress attracted everyone s attention at the party . \n",
            "번역문:  그 여자가 파티에 입고 온 드레스가 사람들의 눈길을 끌었지 . \n",
            "예측문:   톰은 사람은 프랑스어를 안 않아 . <sos> 있어 . <sos> ? <sos> ? \n",
            "\n",
            "\n",
            "원문:  I can t sleep . \n",
            "번역문:  잠이 와 . \n",
            "예측문:   톰은 사람은 프랑스어를 안 수 있어 . <sos> 있어 . <sos> ? <sos> \n",
            "\n",
            "\n",
            "원문:  I looked for the key under the welcome mat and in the nearby flower pot . \n",
            "번역문:  나는 열쇠를 찾기 위해 현관 매트 아래와 근처 화분 안을 뒤졌다 . \n",
            "예측문:   톰은 사람은 프랑스어를 안 않아 . <sos> 있어 . <sos> ? <sos> ? \n",
            "\n",
            "\n",
            "원문:  Tom drove . \n",
            "번역문:  톰이 운전했어 . \n",
            "예측문:   톰은 사람은 프랑스어를 안 수 있어 . <sos> 있어 . <sos> ? <sos> \n",
            "\n",
            "\n",
            "원문:  Her faith in God is unshaken . \n",
            "번역문:  그녀의 신앙심은 굳건하다 . \n",
            "예측문:   톰은 사람은 프랑스어를 안 않아 . <sos> 있어 . <sos> ? <sos> \n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5oDPuAaSIOe2"
      },
      "source": [
        "- 느낀점\n",
        "  - LSTM으로 번역을 하기 매우 어렵다\n",
        "  - 해보았다는 데 의의를 둔다"
      ]
    }
  ]
}